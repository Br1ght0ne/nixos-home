#compdef _rclone rclone


function _rclone {
  local -a commands

  _arguments -C \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '(-V --version)'{-V,--version}'[Print the version number]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    "1: :->cmnds" \
    "*::arg:->args"

  case $state in
  cmnds)
    commands=(
      "about:Get quota information from the remote."
      "authorize:Remote authorization."
      "cachestats:Print cache stats for a remote"
      "cat:Concatenates any files and sends them to stdout."
      "check:Checks the files in the source and destination match."
      "cleanup:Clean up the remote if possible"
      "config:Enter an interactive configuration session."
      "copy:Copy files from source to dest, skipping already copied"
      "copyto:Copy files from source to dest, skipping already copied"
      "copyurl:Copy url content to dest."
      "cryptcheck:Cryptcheck checks the integrity of a crypted remote."
      "cryptdecode:Cryptdecode returns unencrypted file names."
      "dbhashsum:Produces a Dropbox hash file for all the objects in the path."
      "dedupe:Interactively find duplicate files and delete/rename them."
      "delete:Remove the contents of path."
      "deletefile:Remove a single file from remote."
      "genautocomplete:Output completion script for a given shell."
      "gendocs:Output markdown docs for rclone to the directory supplied."
      "hashsum:Produces an hashsum file for all the objects in the path."
      "help:Show help for rclone commands, flags and backends."
      "link:Generate public link to file/folder."
      "listremotes:List all the remotes in the config file."
      "ls:List the objects in the path with size and path."
      "lsd:List all directories/containers/buckets in the path."
      "lsf:List directories and objects in remote:path formatted for parsing"
      "lsjson:List directories and objects in the path in JSON format."
      "lsl:List the objects in path with modification time, size and path."
      "md5sum:Produces an md5sum file for all the objects in the path."
      "mkdir:Make the path if it doesn't already exist."
      "mount:Mount the remote as file system on a mountpoint."
      "move:Move files from source to dest."
      "moveto:Move file or directory from source to dest."
      "ncdu:Explore a remote with a text based user interface."
      "obscure:Obscure password for use in the rclone.conf"
      "purge:Remove the path and all of its contents."
      "rc:Run a command against a running rclone."
      "rcat:Copies standard input to file on remote."
      "rcd:Run rclone listening to remote control commands only."
      "rmdir:Remove the path if empty."
      "rmdirs:Remove empty directories under the path."
      "serve:Serve a remote over a protocol."
      "settier:Changes storage class/tier of objects in remote."
      "sha1sum:Produces an sha1sum file for all the objects in the path."
      "size:Prints the total size and number of objects in remote:path."
      "sync:Make source and dest identical, modifying destination only."
      "touch:Create new file or change file modification time."
      "tree:List the contents of the remote in a tree like fashion."
      "version:Show the version number."
    )
    _describe "command" commands
    ;;
  esac

  case "$words[1]" in
  about)
    _rclone_about
    ;;
  authorize)
    _rclone_authorize
    ;;
  cachestats)
    _rclone_cachestats
    ;;
  cat)
    _rclone_cat
    ;;
  check)
    _rclone_check
    ;;
  cleanup)
    _rclone_cleanup
    ;;
  config)
    _rclone_config
    ;;
  copy)
    _rclone_copy
    ;;
  copyto)
    _rclone_copyto
    ;;
  copyurl)
    _rclone_copyurl
    ;;
  cryptcheck)
    _rclone_cryptcheck
    ;;
  cryptdecode)
    _rclone_cryptdecode
    ;;
  dbhashsum)
    _rclone_dbhashsum
    ;;
  dedupe)
    _rclone_dedupe
    ;;
  delete)
    _rclone_delete
    ;;
  deletefile)
    _rclone_deletefile
    ;;
  genautocomplete)
    _rclone_genautocomplete
    ;;
  gendocs)
    _rclone_gendocs
    ;;
  hashsum)
    _rclone_hashsum
    ;;
  help)
    _rclone_help
    ;;
  link)
    _rclone_link
    ;;
  listremotes)
    _rclone_listremotes
    ;;
  ls)
    _rclone_ls
    ;;
  lsd)
    _rclone_lsd
    ;;
  lsf)
    _rclone_lsf
    ;;
  lsjson)
    _rclone_lsjson
    ;;
  lsl)
    _rclone_lsl
    ;;
  md5sum)
    _rclone_md5sum
    ;;
  mkdir)
    _rclone_mkdir
    ;;
  mount)
    _rclone_mount
    ;;
  move)
    _rclone_move
    ;;
  moveto)
    _rclone_moveto
    ;;
  ncdu)
    _rclone_ncdu
    ;;
  obscure)
    _rclone_obscure
    ;;
  purge)
    _rclone_purge
    ;;
  rc)
    _rclone_rc
    ;;
  rcat)
    _rclone_rcat
    ;;
  rcd)
    _rclone_rcd
    ;;
  rmdir)
    _rclone_rmdir
    ;;
  rmdirs)
    _rclone_rmdirs
    ;;
  serve)
    _rclone_serve
    ;;
  settier)
    _rclone_settier
    ;;
  sha1sum)
    _rclone_sha1sum
    ;;
  size)
    _rclone_size
    ;;
  sync)
    _rclone_sync
    ;;
  touch)
    _rclone_touch
    ;;
  tree)
    _rclone_tree
    ;;
  version)
    _rclone_version
    ;;
  esac
}

function _rclone_about {
  _arguments \
    '--full[Full numbers instead of SI units]' \
    '--json[Format output as JSON]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_authorize {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_cachestats {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_cat {
  _arguments \
    '--count[Only print N characters.]:' \
    '--discard[Discard the output instead of printing.]' \
    '--head[Only print the first N characters.]:' \
    '--offset[Start printing at offset N (or from end if -ve).]:' \
    '--tail[Only print the last N characters.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_check {
  _arguments \
    '--download[Check by downloading rather than with hash.]' \
    '--one-way[Check one way only, source files must exist on remote]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_cleanup {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}


function _rclone_config {
  local -a commands

  _arguments -C \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    "1: :->cmnds" \
    "*::arg:->args"

  case $state in
  cmnds)
    commands=(
      "create:Create a new remote with name, type and options."
      "delete:Delete an existing remote <name>."
      "disconnect:Disconnects user from remote"
      "dump:Dump the config file as JSON."
      "edit:Enter an interactive configuration session."
      "file:Show path of configuration file in use."
      "password:Update password in an existing remote."
      "providers:List in JSON format all the providers and options."
      "reconnect:Re-authenticates user with remote."
      "show:Print (decrypted) config file, or the config for a single remote."
      "update:Update options in an existing remote."
      "userinfo:Prints info about logged in user of remote."
    )
    _describe "command" commands
    ;;
  esac

  case "$words[1]" in
  create)
    _rclone_config_create
    ;;
  delete)
    _rclone_config_delete
    ;;
  disconnect)
    _rclone_config_disconnect
    ;;
  dump)
    _rclone_config_dump
    ;;
  edit)
    _rclone_config_edit
    ;;
  file)
    _rclone_config_file
    ;;
  password)
    _rclone_config_password
    ;;
  providers)
    _rclone_config_providers
    ;;
  reconnect)
    _rclone_config_reconnect
    ;;
  show)
    _rclone_config_show
    ;;
  update)
    _rclone_config_update
    ;;
  userinfo)
    _rclone_config_userinfo
    ;;
  esac
}

function _rclone_config_create {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_delete {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_disconnect {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_dump {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_edit {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_file {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_password {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_providers {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_reconnect {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_show {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_update {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_config_userinfo {
  _arguments \
    '--json[Format output as JSON]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_copy {
  _arguments \
    '--create-empty-src-dirs[Create empty source dirs on destination after copy]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_copyto {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_copyurl {
  _arguments \
    '(-a --auto-filename)'{-a,--auto-filename}'[Get the file name from the url and use it for destination file path]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_cryptcheck {
  _arguments \
    '--one-way[Check one way only, source files must exist on destination]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_cryptdecode {
  _arguments \
    '--reverse[Reverse cryptdecode, encrypts filenames]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_dbhashsum {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_dedupe {
  _arguments \
    '--dedupe-mode[Dedupe mode interactive|skip|first|newest|oldest|rename.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_delete {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_deletefile {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}


function _rclone_genautocomplete {
  local -a commands

  _arguments -C \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    "1: :->cmnds" \
    "*::arg:->args"

  case $state in
  cmnds)
    commands=(
      "bash:Output bash completion script for rclone."
      "zsh:Output zsh completion script for rclone."
    )
    _describe "command" commands
    ;;
  esac

  case "$words[1]" in
  bash)
    _rclone_genautocomplete_bash
    ;;
  zsh)
    _rclone_genautocomplete_zsh
    ;;
  esac
}

function _rclone_genautocomplete_bash {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_genautocomplete_zsh {
  _arguments \
    '(-h --help)'{-h,--help}'[help for zsh]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_gendocs {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_hashsum {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}


function _rclone_help {
  local -a commands

  _arguments -C \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    "1: :->cmnds" \
    "*::arg:->args"

  case $state in
  cmnds)
    commands=(
      "backend:List full info about a backend"
      "backends:List the backends available"
      "flags:Show the global flags for rclone"
    )
    _describe "command" commands
    ;;
  esac

  case "$words[1]" in
  backend)
    _rclone_help_backend
    ;;
  backends)
    _rclone_help_backends
    ;;
  flags)
    _rclone_help_flags
    ;;
  esac
}

function _rclone_help_backend {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_help_backends {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_help_flags {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_link {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_listremotes {
  _arguments \
    '--long[Show the type as well as names.]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_ls {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_lsd {
  _arguments \
    '(-R --recursive)'{-R,--recursive}'[Recurse into the listing.]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_lsf {
  _arguments \
    '--absolute[Put a leading / in front of path names.]' \
    '--csv[Output in CSV format.]' \
    '(-d --dir-slash)'{-d,--dir-slash}'[Append a slash to directory names.]' \
    '--dirs-only[Only list directories.]' \
    '--files-only[Only list files.]' \
    '(-F --format)'{-F,--format}'[Output format - see  help for details]:' \
    '--hash[Use this hash when `h` is used in the format MD5|SHA-1|DropboxHash]:' \
    '(-R --recursive)'{-R,--recursive}'[Recurse into the listing.]' \
    '(-s --separator)'{-s,--separator}'[Separator for the items in the format.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_lsjson {
  _arguments \
    '--dirs-only[Show only directories in the listing.]' \
    '(-M --encrypted)'{-M,--encrypted}'[Show the encrypted names.]' \
    '--files-only[Show only files in the listing.]' \
    '--hash[Include hashes in the output (may take longer).]' \
    '--no-modtime[Don'\''t read the modification time (can speed things up).]' \
    '--original[Show the ID of the underlying Object.]' \
    '(-R --recursive)'{-R,--recursive}'[Recurse into the listing.]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_lsl {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_md5sum {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_mkdir {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_mount {
  _arguments \
    '--allow-non-empty[Allow mounting over a non-empty directory.]' \
    '--allow-other[Allow access to other users.]' \
    '--allow-root[Allow access to root user.]' \
    '--attr-timeout[Time for which file/directory attributes are cached.]:' \
    '--daemon[Run mount as a daemon (background mode).]' \
    '--daemon-timeout[Time limit for rclone to respond to kernel (not supported by all OSes).]:' \
    '--debug-fuse[Debug the FUSE internals - needs -v.]' \
    '--default-permissions[Makes kernel enforce access control based on the file mode.]' \
    '--dir-cache-time[Time to cache directory entries for.]:' \
    '--dir-perms[Directory permissions]:' \
    '--file-perms[File permissions]:' \
    '*--fuse-flag[Flags or arguments to be passed direct to libfuse/WinFsp. Repeat if required.]:' \
    '--gid[Override the gid field set by the filesystem.]:' \
    '--max-read-ahead[The number of bytes that can be prefetched for sequential reads.]:' \
    '--no-checksum[Don'\''t compare checksums on up/download.]' \
    '--no-modtime[Don'\''t read/write the modification time (can speed things up).]' \
    '--no-seek[Don'\''t allow seeking in files.]' \
    '(*-o *--option)'{\*-o,\*--option}'[Option for libfuse/WinFsp. Repeat if required.]:' \
    '--poll-interval[Time to wait between polling for changes. Must be smaller than dir-cache-time. Only on supported remotes. Set to 0 to disable.]:' \
    '--read-only[Mount read-only.]' \
    '--uid[Override the uid field set by the filesystem.]:' \
    '--umask[Override the permission bits set by the filesystem.]:' \
    '--vfs-cache-max-age[Max age of objects in the cache.]:' \
    '--vfs-cache-max-size[Max total size of objects in the cache.]:' \
    '--vfs-cache-mode[Cache mode off|minimal|writes|full]:' \
    '--vfs-cache-poll-interval[Interval to poll the cache for stale objects.]:' \
    '--vfs-case-insensitive[If a file name not found, find a case insensitive match.]' \
    '--vfs-read-chunk-size[Read the source objects in chunks.]:' \
    '--vfs-read-chunk-size-limit[If greater than --vfs-read-chunk-size, double the chunk size after each chunk read, until the limit is reached. '\''off'\'' is unlimited.]:' \
    '--volname[Set the volume name (not supported by all OSes).]:' \
    '--write-back-cache[Makes kernel buffer writes before sending them to rclone. Without this, writethrough caching is used.]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_move {
  _arguments \
    '--create-empty-src-dirs[Create empty source dirs on destination after move]' \
    '--delete-empty-src-dirs[Delete empty source dirs after move]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_moveto {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_ncdu {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_obscure {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_purge {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_rc {
  _arguments \
    '--json[Input JSON - use instead of key=value args.]:' \
    '--loopback[If set connect to this rclone instance not via HTTP.]' \
    '--no-output[If set don'\''t output the JSON result.]' \
    '--pass[Password to use to connect to rclone remote control.]:' \
    '--url[URL to connect to rclone remote control.]:' \
    '--user[Username to use to rclone remote control.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_rcat {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_rcd {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_rmdir {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_rmdirs {
  _arguments \
    '--leave-root[Do not remove root directory if empty]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}


function _rclone_serve {
  local -a commands

  _arguments -C \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    "1: :->cmnds" \
    "*::arg:->args"

  case $state in
  cmnds)
    commands=(
      "dlna:Serve remote:path over DLNA"
      "ftp:Serve remote:path over FTP."
      "http:Serve the remote over HTTP."
      "restic:Serve the remote for restic's REST API."
      "sftp:Serve the remote over SFTP."
      "webdav:Serve remote:path over webdav."
    )
    _describe "command" commands
    ;;
  esac

  case "$words[1]" in
  dlna)
    _rclone_serve_dlna
    ;;
  ftp)
    _rclone_serve_ftp
    ;;
  http)
    _rclone_serve_http
    ;;
  restic)
    _rclone_serve_restic
    ;;
  sftp)
    _rclone_serve_sftp
    ;;
  webdav)
    _rclone_serve_webdav
    ;;
  esac
}

function _rclone_serve_dlna {
  _arguments \
    '--addr[ip:port or :port to bind the DLNA http server to.]:' \
    '--dir-cache-time[Time to cache directory entries for.]:' \
    '--dir-perms[Directory permissions]:' \
    '--file-perms[File permissions]:' \
    '--gid[Override the gid field set by the filesystem.]:' \
    '--log-trace[enable trace logging of SOAP traffic]' \
    '--name[name of DLNA server]:' \
    '--no-checksum[Don'\''t compare checksums on up/download.]' \
    '--no-modtime[Don'\''t read/write the modification time (can speed things up).]' \
    '--no-seek[Don'\''t allow seeking in files.]' \
    '--poll-interval[Time to wait between polling for changes. Must be smaller than dir-cache-time. Only on supported remotes. Set to 0 to disable.]:' \
    '--read-only[Mount read-only.]' \
    '--uid[Override the uid field set by the filesystem.]:' \
    '--umask[Override the permission bits set by the filesystem.]:' \
    '--vfs-cache-max-age[Max age of objects in the cache.]:' \
    '--vfs-cache-max-size[Max total size of objects in the cache.]:' \
    '--vfs-cache-mode[Cache mode off|minimal|writes|full]:' \
    '--vfs-cache-poll-interval[Interval to poll the cache for stale objects.]:' \
    '--vfs-case-insensitive[If a file name not found, find a case insensitive match.]' \
    '--vfs-read-chunk-size[Read the source objects in chunks.]:' \
    '--vfs-read-chunk-size-limit[If greater than --vfs-read-chunk-size, double the chunk size after each chunk read, until the limit is reached. '\''off'\'' is unlimited.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_serve_ftp {
  _arguments \
    '--addr[IPaddress:Port or :Port to bind server to.]:' \
    '--auth-proxy[A program to use to create the backend from the auth.]:' \
    '--dir-cache-time[Time to cache directory entries for.]:' \
    '--dir-perms[Directory permissions]:' \
    '--file-perms[File permissions]:' \
    '--gid[Override the gid field set by the filesystem.]:' \
    '--no-checksum[Don'\''t compare checksums on up/download.]' \
    '--no-modtime[Don'\''t read/write the modification time (can speed things up).]' \
    '--no-seek[Don'\''t allow seeking in files.]' \
    '--pass[Password for authentication. (empty value allow every password)]:' \
    '--passive-port[Passive port range to use.]:' \
    '--poll-interval[Time to wait between polling for changes. Must be smaller than dir-cache-time. Only on supported remotes. Set to 0 to disable.]:' \
    '--public-ip[Public IP address to advertise for passive connections.]:' \
    '--read-only[Mount read-only.]' \
    '--uid[Override the uid field set by the filesystem.]:' \
    '--umask[Override the permission bits set by the filesystem.]:' \
    '--user[User name for authentication.]:' \
    '--vfs-cache-max-age[Max age of objects in the cache.]:' \
    '--vfs-cache-max-size[Max total size of objects in the cache.]:' \
    '--vfs-cache-mode[Cache mode off|minimal|writes|full]:' \
    '--vfs-cache-poll-interval[Interval to poll the cache for stale objects.]:' \
    '--vfs-case-insensitive[If a file name not found, find a case insensitive match.]' \
    '--vfs-read-chunk-size[Read the source objects in chunks.]:' \
    '--vfs-read-chunk-size-limit[If greater than --vfs-read-chunk-size, double the chunk size after each chunk read, until the limit is reached. '\''off'\'' is unlimited.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_serve_http {
  _arguments \
    '--addr[IPaddress:Port or :Port to bind server to.]:' \
    '--baseurl[Prefix for URLs - leave blank for root.]:' \
    '--cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--client-ca[Client certificate authority to verify clients with]:' \
    '--dir-cache-time[Time to cache directory entries for.]:' \
    '--dir-perms[Directory permissions]:' \
    '--file-perms[File permissions]:' \
    '--gid[Override the gid field set by the filesystem.]:' \
    '--htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--key[SSL PEM Private key]:' \
    '--max-header-bytes[Maximum size of request header]:' \
    '--no-checksum[Don'\''t compare checksums on up/download.]' \
    '--no-modtime[Don'\''t read/write the modification time (can speed things up).]' \
    '--no-seek[Don'\''t allow seeking in files.]' \
    '--pass[Password for authentication.]:' \
    '--poll-interval[Time to wait between polling for changes. Must be smaller than dir-cache-time. Only on supported remotes. Set to 0 to disable.]:' \
    '--read-only[Mount read-only.]' \
    '--realm[realm for authentication]:' \
    '--server-read-timeout[Timeout for server reading data]:' \
    '--server-write-timeout[Timeout for server writing data]:' \
    '--uid[Override the uid field set by the filesystem.]:' \
    '--umask[Override the permission bits set by the filesystem.]:' \
    '--user[User name for authentication.]:' \
    '--vfs-cache-max-age[Max age of objects in the cache.]:' \
    '--vfs-cache-max-size[Max total size of objects in the cache.]:' \
    '--vfs-cache-mode[Cache mode off|minimal|writes|full]:' \
    '--vfs-cache-poll-interval[Interval to poll the cache for stale objects.]:' \
    '--vfs-case-insensitive[If a file name not found, find a case insensitive match.]' \
    '--vfs-read-chunk-size[Read the source objects in chunks.]:' \
    '--vfs-read-chunk-size-limit[If greater than --vfs-read-chunk-size, double the chunk size after each chunk read, until the limit is reached. '\''off'\'' is unlimited.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_serve_restic {
  _arguments \
    '--addr[IPaddress:Port or :Port to bind server to.]:' \
    '--append-only[disallow deletion of repository data]' \
    '--baseurl[Prefix for URLs - leave blank for root.]:' \
    '--cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--client-ca[Client certificate authority to verify clients with]:' \
    '--htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--key[SSL PEM Private key]:' \
    '--max-header-bytes[Maximum size of request header]:' \
    '--pass[Password for authentication.]:' \
    '--private-repos[users can only access their private repo]' \
    '--realm[realm for authentication]:' \
    '--server-read-timeout[Timeout for server reading data]:' \
    '--server-write-timeout[Timeout for server writing data]:' \
    '--stdio[run an HTTP2 server on stdin/stdout]' \
    '--user[User name for authentication.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_serve_sftp {
  _arguments \
    '--addr[IPaddress:Port or :Port to bind server to.]:' \
    '--auth-proxy[A program to use to create the backend from the auth.]:' \
    '--authorized-keys[Authorized keys file]:' \
    '--dir-cache-time[Time to cache directory entries for.]:' \
    '--dir-perms[Directory permissions]:' \
    '--file-perms[File permissions]:' \
    '--gid[Override the gid field set by the filesystem.]:' \
    '--key[SSH private key file (leave blank to auto generate)]:' \
    '--no-auth[Allow connections with no authentication if set.]' \
    '--no-checksum[Don'\''t compare checksums on up/download.]' \
    '--no-modtime[Don'\''t read/write the modification time (can speed things up).]' \
    '--no-seek[Don'\''t allow seeking in files.]' \
    '--pass[Password for authentication.]:' \
    '--poll-interval[Time to wait between polling for changes. Must be smaller than dir-cache-time. Only on supported remotes. Set to 0 to disable.]:' \
    '--read-only[Mount read-only.]' \
    '--uid[Override the uid field set by the filesystem.]:' \
    '--umask[Override the permission bits set by the filesystem.]:' \
    '--user[User name for authentication.]:' \
    '--vfs-cache-max-age[Max age of objects in the cache.]:' \
    '--vfs-cache-max-size[Max total size of objects in the cache.]:' \
    '--vfs-cache-mode[Cache mode off|minimal|writes|full]:' \
    '--vfs-cache-poll-interval[Interval to poll the cache for stale objects.]:' \
    '--vfs-case-insensitive[If a file name not found, find a case insensitive match.]' \
    '--vfs-read-chunk-size[Read the source objects in chunks.]:' \
    '--vfs-read-chunk-size-limit[If greater than --vfs-read-chunk-size, double the chunk size after each chunk read, until the limit is reached. '\''off'\'' is unlimited.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_serve_webdav {
  _arguments \
    '--addr[IPaddress:Port or :Port to bind server to.]:' \
    '--auth-proxy[A program to use to create the backend from the auth.]:' \
    '--baseurl[Prefix for URLs - leave blank for root.]:' \
    '--cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--client-ca[Client certificate authority to verify clients with]:' \
    '--dir-cache-time[Time to cache directory entries for.]:' \
    '--dir-perms[Directory permissions]:' \
    '--disable-dir-list[Disable HTML directory list on GET request for a directory]' \
    '--etag-hash[Which hash to use for the ETag, or auto or blank for off]:' \
    '--file-perms[File permissions]:' \
    '--gid[Override the gid field set by the filesystem.]:' \
    '--htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--key[SSL PEM Private key]:' \
    '--max-header-bytes[Maximum size of request header]:' \
    '--no-checksum[Don'\''t compare checksums on up/download.]' \
    '--no-modtime[Don'\''t read/write the modification time (can speed things up).]' \
    '--no-seek[Don'\''t allow seeking in files.]' \
    '--pass[Password for authentication.]:' \
    '--poll-interval[Time to wait between polling for changes. Must be smaller than dir-cache-time. Only on supported remotes. Set to 0 to disable.]:' \
    '--read-only[Mount read-only.]' \
    '--realm[realm for authentication]:' \
    '--server-read-timeout[Timeout for server reading data]:' \
    '--server-write-timeout[Timeout for server writing data]:' \
    '--uid[Override the uid field set by the filesystem.]:' \
    '--umask[Override the permission bits set by the filesystem.]:' \
    '--user[User name for authentication.]:' \
    '--vfs-cache-max-age[Max age of objects in the cache.]:' \
    '--vfs-cache-max-size[Max total size of objects in the cache.]:' \
    '--vfs-cache-mode[Cache mode off|minimal|writes|full]:' \
    '--vfs-cache-poll-interval[Interval to poll the cache for stale objects.]:' \
    '--vfs-case-insensitive[If a file name not found, find a case insensitive match.]' \
    '--vfs-read-chunk-size[Read the source objects in chunks.]:' \
    '--vfs-read-chunk-size-limit[If greater than --vfs-read-chunk-size, double the chunk size after each chunk read, until the limit is reached. '\''off'\'' is unlimited.]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_settier {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_sha1sum {
  _arguments \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_size {
  _arguments \
    '--json[format output as JSON]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_sync {
  _arguments \
    '--create-empty-src-dirs[Create empty source dirs on destination after sync]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_touch {
  _arguments \
    '(-C --no-create)'{-C,--no-create}'[Do not create the file if it does not exist.]' \
    '(-t --timestamp)'{-t,--timestamp}'[Change the modification times to the specified time instead of the current time of day. The argument is of the form '\''YYMMDD'\'' (ex. 17.10.30) or '\''YYYY-MM-DDTHH:MM:SS'\'' (ex. 2006-01-02T15:04:05)]:' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_tree {
  _arguments \
    '(-a --all)'{-a,--all}'[All files are listed (list . files too).]' \
    '(-C --color)'{-C,--color}'[Turn colorization on always.]' \
    '(-d --dirs-only)'{-d,--dirs-only}'[List directories only.]' \
    '--dirsfirst[List directories before files (-U disables).]' \
    '--full-path[Print the full path prefix for each file.]' \
    '--human[Print the size in a more human readable way.]' \
    '--level[Descend only level directories deep.]:' \
    '(-D --modtime)'{-D,--modtime}'[Print the date of last modification.]' \
    '(-i --noindent)'{-i,--noindent}'[Don'\''t print indentation lines.]' \
    '--noreport[Turn off file/directory count at end of tree listing.]' \
    '(-o --output)'{-o,--output}'[Output to file instead of stdout.]:' \
    '(-p --protections)'{-p,--protections}'[Print the protections for each file.]' \
    '(-Q --quote)'{-Q,--quote}'[Quote filenames with double quotes.]' \
    '(-s --size)'{-s,--size}'[Print the size in bytes of each file.]' \
    '--sort[Select sort: name,version,size,mtime,ctime.]:' \
    '--sort-ctime[Sort files by last status change time.]' \
    '(-t --sort-modtime)'{-t,--sort-modtime}'[Sort files by last modification time.]' \
    '(-r --sort-reverse)'{-r,--sort-reverse}'[Reverse the order of the sort.]' \
    '(-U --unsorted)'{-U,--unsorted}'[Leave files unsorted.]' \
    '--version[Sort files alphanumerically by version.]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

function _rclone_version {
  _arguments \
    '--check[Check for new version.]' \
    '--acd-auth-url[Auth server URL.]:' \
    '--acd-client-id[Amazon Application Client ID.]:' \
    '--acd-client-secret[Amazon Application Client Secret.]:' \
    '--acd-templink-threshold[Files >= this size will be downloaded via their tempLink.]:' \
    '--acd-token-url[Token server url.]:' \
    '--acd-upload-wait-per-gb[Additional time per GB to wait after a failed complete upload to see if it appears.]:' \
    '--alias-remote[Remote or path to alias.]:' \
    '--ask-password[Allow prompt for password for encrypted configuration.]' \
    '--auto-confirm[If enabled, do not request console confirmation.]' \
    '--azureblob-access-tier[Access tier of blob: hot, cool or archive.]:' \
    '--azureblob-account[Storage Account Name (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-chunk-size[Upload chunk size (<= 100MB).]:' \
    '--azureblob-endpoint[Endpoint for the service]:' \
    '--azureblob-key[Storage Account Key (leave blank to use SAS URL or Emulator)]:' \
    '--azureblob-list-chunk[Size of blob list.]:' \
    '--azureblob-sas-url[SAS URL for container level access only]:' \
    '--azureblob-upload-cutoff[Cutoff for switching to chunked upload (<= 256MB).]:' \
    '--azureblob-use-emulator[Uses local storage emulator if provided as '\''true'\'' (leave blank if using real azure storage endpoint)]' \
    '--b2-account[Account ID or Application Key ID]:' \
    '--b2-chunk-size[Upload chunk size. Must fit in memory.]:' \
    '--b2-disable-checksum[Disable checksums for large (> upload cutoff) files]' \
    '--b2-download-auth-duration[Time before the authorization token will expire in s or suffix ms|s|m|h|d.]:' \
    '--b2-download-url[Custom endpoint for downloads.]:' \
    '--b2-endpoint[Endpoint for the service.]:' \
    '--b2-hard-delete[Permanently delete files on remote removal, otherwise hide files.]' \
    '--b2-key[Application Key]:' \
    '--b2-test-mode[A flag string for X-Bz-Test-Mode header for debugging.]:' \
    '--b2-upload-cutoff[Cutoff for switching to chunked upload.]:' \
    '--b2-versions[Include old versions in directory listings.]' \
    '--backup-dir[Make backups into hierarchy based in DIR.]:' \
    '--bind[Local address to bind to for outgoing connections, IPv4, IPv6 or name.]:' \
    '--box-box-config-file[Box App config.json location]:' \
    '--box-box-sub-type[]:' \
    '--box-client-id[Box App Client Id.]:' \
    '--box-client-secret[Box App Client Secret]:' \
    '--box-commit-retries[Max number of times to try committing a multipart file.]:' \
    '--box-upload-cutoff[Cutoff for switching to multipart upload (>= 50MB).]:' \
    '--buffer-size[In memory buffer size when reading files for each --transfer.]:' \
    '--bwlimit[Bandwidth limit in kBytes/s, or use suffix b|k|M|G or a full timetable.]:' \
    '--ca-cert[CA certificate used to verify servers]:' \
    '--cache-chunk-clean-interval[How often should the cache perform cleanups of the chunk storage.]:' \
    '--cache-chunk-no-memory[Disable the in-memory cache for storing chunks during streaming.]' \
    '--cache-chunk-path[Directory to cache chunk files.]:' \
    '--cache-chunk-size[The size of a chunk (partial file data).]:' \
    '--cache-chunk-total-size[The total size that the chunks can take up on the local disk.]:' \
    '--cache-db-path[Directory to store file structure metadata DB.]:' \
    '--cache-db-purge[Clear all the cached data for this remote on start.]' \
    '--cache-db-wait-time[How long to wait for the DB to be available - 0 is unlimited]:' \
    '--cache-dir[Directory rclone will use for caching.]:' \
    '--cache-info-age[How long to cache file structure information (directory listings, file size, times etc).]:' \
    '--cache-plex-insecure[Skip all certificate verifications when connecting to the Plex server]:' \
    '--cache-plex-password[The password of the Plex user]:' \
    '--cache-plex-url[The URL of the Plex server]:' \
    '--cache-plex-username[The username of the Plex user]:' \
    '--cache-read-retries[How many times to retry a read from a cache storage.]:' \
    '--cache-remote[Remote to cache.]:' \
    '--cache-rps[Limits the number of requests per second to the source FS (-1 to disable)]:' \
    '--cache-tmp-upload-path[Directory to keep temporary files until they are uploaded.]:' \
    '--cache-tmp-wait-time[How long should files be stored in local cache before being uploaded]:' \
    '--cache-workers[How many workers should run in parallel to download chunks.]:' \
    '--cache-writes[Cache file data on writes through the FS]' \
    '--checkers[Number of checkers to run in parallel.]:' \
    '(-c --checksum)'{-c,--checksum}'[Skip based on checksum (if available) & size, not mod-time & size]' \
    '--chunker-chunk-size[Files larger than chunk size will be split in chunks.]:' \
    '--chunker-fail-hard[Choose how chunker should handle files with missing or invalid chunks.]' \
    '--chunker-hash-type[Choose how chunker handles hash sums. All modes but "none" require metadata.]:' \
    '--chunker-meta-format[Format of the metadata object or "none". By default "simplejson".]:' \
    '--chunker-name-format[String format of chunk file names.]:' \
    '--chunker-remote[Remote to chunk/unchunk.]:' \
    '--chunker-start-from[Minimum valid chunk number. Usually 0 or 1.]:' \
    '--client-cert[Client SSL certificate (PEM) for mutual TLS auth]:' \
    '--client-key[Client SSL private key (PEM) for mutual TLS auth]:' \
    '--compare-dest[use DIR to server side copy flies from.]:' \
    '--config[Config file.]:' \
    '--contimeout[Connect timeout]:' \
    '--copy-dest[Compare dest to DIR also.]:' \
    '(-L --copy-links)'{-L,--copy-links}'[Follow symlinks and copy the pointed to item.]' \
    '--cpuprofile[Write cpu profile to file]:' \
    '--crypt-directory-name-encryption[Option to either encrypt directory names or leave them intact.]' \
    '--crypt-filename-encryption[How to encrypt the filenames.]:' \
    '--crypt-password[Password or pass phrase for encryption.]:' \
    '--crypt-password2[Password or pass phrase for salt. Optional but recommended.]:' \
    '--crypt-remote[Remote to encrypt/decrypt.]:' \
    '--crypt-show-mapping[For all files listed show how the names encrypt.]' \
    '--delete-after[When synchronizing, delete files on destination after transferring (default)]' \
    '--delete-before[When synchronizing, delete files on destination before transferring]' \
    '--delete-during[When synchronizing, delete files during transfer]' \
    '--delete-excluded[Delete files on dest excluded from sync]' \
    '--disable[Disable a comma separated list of features.  Use help to see a list.]:' \
    '--drive-acknowledge-abuse[Set to allow files which return cannotDownloadAbusiveFile to be downloaded.]' \
    '--drive-allow-import-name-change[Allow the filetype to change when uploading Google docs (e.g. file.doc to file.docx). This will confuse sync and reupload every time.]' \
    '--drive-alternate-export[Use alternate export URLs for google documents export.,]' \
    '--drive-auth-owner-only[Only consider files owned by the authenticated user.]' \
    '--drive-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--drive-client-id[Google Application Client Id]:' \
    '--drive-client-secret[Google Application Client Secret]:' \
    '--drive-disable-http2[Disable drive using http2]' \
    '--drive-export-formats[Comma separated list of preferred formats for downloading Google docs.]:' \
    '--drive-formats[Deprecated: see export_formats]:' \
    '--drive-impersonate[Impersonate this user when using a service account.]:' \
    '--drive-import-formats[Comma separated list of preferred formats for uploading Google docs.]:' \
    '--drive-keep-revision-forever[Keep new head revision of each file forever.]' \
    '--drive-list-chunk[Size of listing chunk 100-1000. 0 to disable.]:' \
    '--drive-pacer-burst[Number of API calls to allow without sleeping.]:' \
    '--drive-pacer-min-sleep[Minimum time to sleep between API calls.]:' \
    '--drive-root-folder-id[ID of the root folder]:' \
    '--drive-scope[Scope that rclone should use when requesting access from drive.]:' \
    '--drive-server-side-across-configs[Allow server side operations (eg copy) to work across different drive configs.]' \
    '--drive-service-account-credentials[Service Account Credentials JSON blob]:' \
    '--drive-service-account-file[Service Account Credentials JSON file path]:' \
    '--drive-shared-with-me[Only show files that are shared with me.]' \
    '--drive-size-as-quota[Show storage quota usage for file size.]' \
    '--drive-skip-checksum-gphotos[Skip MD5 checksum on Google photos and videos only.]' \
    '--drive-skip-gdocs[Skip google documents in all listings.]' \
    '--drive-team-drive[ID of the Team Drive]:' \
    '--drive-trashed-only[Only show files that are in the trash.]' \
    '--drive-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--drive-use-created-date[Use file created date instead of modified date.,]' \
    '--drive-use-trash[Send files to the trash instead of deleting permanently.]' \
    '--drive-v2-download-min-size[If Object'\''s are greater, use drive v2 API to download.]:' \
    '--dropbox-chunk-size[Upload chunk size. (< 150M).]:' \
    '--dropbox-client-id[Dropbox App Client Id]:' \
    '--dropbox-client-secret[Dropbox App Client Secret]:' \
    '--dropbox-impersonate[Impersonate this user when using a business account.]:' \
    '(-n --dry-run)'{-n,--dry-run}'[Do a trial run with no permanent changes]' \
    '--dump[List of items to dump from: headers,bodies,requests,responses,auth,filters,goroutines,openfiles]:' \
    '--dump-bodies[Dump HTTP headers and bodies - may contain sensitive info]' \
    '--dump-headers[Dump HTTP headers - may contain sensitive info]' \
    '*--exclude[Exclude files matching pattern]:' \
    '*--exclude-from[Read exclude patterns from file]:' \
    '--exclude-if-present[Exclude directories if filename is present]:' \
    '--fast-list[Use recursive list if available. Uses more memory but fewer transactions.]' \
    '--fichier-api-key[Your API Key, get it from https://1fichier.com/console/params.pl]:' \
    '--fichier-shared-folder[If you want to download a shared folder, add this parameter]:' \
    '*--files-from[Read list of source-file names from file]:' \
    '(*-f *--filter)'{\*-f,\*--filter}'[Add a file-filtering rule]:' \
    '*--filter-from[Read filtering patterns from a file]:' \
    '--ftp-concurrency[Maximum number of FTP simultaneous connections, 0 for unlimited]:' \
    '--ftp-disable-epsv[Disable using EPSV even if server advertises support]' \
    '--ftp-host[FTP host to connect to]:' \
    '--ftp-no-check-certificate[Do not verify the TLS certificate of the server]' \
    '--ftp-pass[FTP password]:' \
    '--ftp-port[FTP port, leave blank to use default (21)]:' \
    '--ftp-tls[Use FTP over TLS (Implicit)]' \
    '--ftp-user[FTP username, leave blank for current username, brightone]:' \
    '--gcs-bucket-acl[Access Control List for new buckets.]:' \
    '--gcs-bucket-policy-only[Access checks should use bucket-level IAM policies.]' \
    '--gcs-client-id[Google Application Client Id]:' \
    '--gcs-client-secret[Google Application Client Secret]:' \
    '--gcs-location[Location for the newly created buckets.]:' \
    '--gcs-object-acl[Access Control List for new objects.]:' \
    '--gcs-project-number[Project number.]:' \
    '--gcs-service-account-file[Service Account Credentials JSON file path]:' \
    '--gcs-storage-class[The storage class to use when storing objects in Google Cloud Storage.]:' \
    '--gphotos-client-id[Google Application Client Id]:' \
    '--gphotos-client-secret[Google Application Client Secret]:' \
    '--gphotos-read-only[Set to make the Google Photos backend read only.]' \
    '--gphotos-read-size[Set to read the size of media items.]' \
    '--http-headers[Set HTTP headers for all transactions]:' \
    '--http-no-head[Don'\''t use HEAD requests to find file sizes in dir listing]' \
    '--http-no-slash[Set this if the site doesn'\''t end directories with /]' \
    '--http-url[URL of http host to connect to]:' \
    '--hubic-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--hubic-client-id[Hubic Client Id]:' \
    '--hubic-client-secret[Hubic Client Secret]:' \
    '--hubic-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--ignore-case[Ignore case in filters (case insensitive)]' \
    '--ignore-case-sync[Ignore case when synchronizing]' \
    '--ignore-checksum[Skip post copy check of checksums.]' \
    '--ignore-errors[delete even if there are I/O errors]' \
    '--ignore-existing[Skip all files that exist on destination]' \
    '--ignore-size[Ignore size when skipping use mod-time or checksum.]' \
    '(-I --ignore-times)'{-I,--ignore-times}'[Don'\''t skip files that match size and time - transfer all files]' \
    '--immutable[Do not modify files. Fail if existing files have been modified.]' \
    '*--include[Include files matching pattern]:' \
    '*--include-from[Read include patterns from file]:' \
    '--jottacloud-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--jottacloud-md5-memory-limit[Files bigger than this will be cached on disk to calculate the MD5 if required.]:' \
    '--jottacloud-unlink[Remove existing public link to file/folder with link command rather than creating.]' \
    '--jottacloud-upload-resume-limit[Files bigger than this can be resumed if the upload fail'\''s.]:' \
    '--koofr-endpoint[The Koofr API endpoint to use]:' \
    '--koofr-mountid[Mount ID of the mount to use. If omitted, the primary mount is used.]:' \
    '--koofr-password[Your Koofr password for rclone (generate one at https://app.koofr.net/app/admin/preferences/password)]:' \
    '--koofr-setmtime[Does the backend support setting modification time. Set this to false if you use a mount ID that points to a Dropbox or Amazon Drive backend.]' \
    '--koofr-user[Your Koofr user name]:' \
    '(-l --links)'{-l,--links}'[Translate symlinks to/from regular files with a '\''.rclonelink'\'' extension]' \
    '--local-case-insensitive[Force the filesystem to report itself as case insensitive]' \
    '--local-case-sensitive[Force the filesystem to report itself as case sensitive.]' \
    '--local-no-check-updated[Don'\''t check to see if the files change during upload]' \
    '--local-no-unicode-normalization[Don'\''t apply unicode normalization to paths and filenames (Deprecated)]' \
    '--local-nounc[Disable UNC (long path names) conversion on Windows]:' \
    '--log-file[Log everything to this file]:' \
    '--log-format[Comma separated list of log format options]:' \
    '--log-level[Log level DEBUG|INFO|NOTICE|ERROR]:' \
    '--low-level-retries[Number of low level retries to do.]:' \
    '--mailru-check-hash[What should copy do if file checksum is mismatched or invalid]' \
    '--mailru-pass[Password]:' \
    '--mailru-speedup-enable[Skip full upload if there is another file with same data hash.]' \
    '--mailru-speedup-file-patterns[Comma separated list of file name patterns eligible for speedup (put by hash).]:' \
    '--mailru-speedup-max-disk[This option allows you to disable speedup (put by hash) for large files]:' \
    '--mailru-speedup-max-memory[Files larger than the size given below will always be hashed on disk.]:' \
    '--mailru-user[User name (usually email)]:' \
    '--max-age[Only transfer files younger than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--max-backlog[Maximum number of objects in sync or check backlog.]:' \
    '--max-delete[When synchronizing, limit the number of deletes]:' \
    '--max-depth[If set limits the recursion depth to this.]:' \
    '--max-size[Only transfer files smaller than this in k or suffix b|k|M|G]:' \
    '--max-stats-groups[Maximum number of stats groups to keep in memory. On max oldest is discarded.]:' \
    '--max-transfer[Maximum size of data to transfer.]:' \
    '--mega-debug[Output more debug from Mega.]' \
    '--mega-hard-delete[Delete files permanently rather than putting them into the trash.]' \
    '--mega-pass[Password.]:' \
    '--mega-user[User name]:' \
    '--memprofile[Write memory profile to file]:' \
    '--min-age[Only transfer files older than this in s or suffix ms|s|m|h|d|w|M|y]:' \
    '--min-size[Only transfer files bigger than this in k or suffix b|k|M|G]:' \
    '--modify-window[Max time diff to be considered the same]:' \
    '--multi-thread-cutoff[Use multi-thread downloads for files above this size.]:' \
    '--multi-thread-streams[Max number of streams to use for multi-thread downloads.]:' \
    '--no-check-certificate[Do not verify the server SSL certificate. Insecure.]' \
    '--no-gzip-encoding[Don'\''t set Accept-Encoding: gzip.]' \
    '--no-traverse[Don'\''t traverse destination file system on copy.]' \
    '--no-update-modtime[Don'\''t update destination mod-time if files identical.]' \
    '(-x --one-file-system)'{-x,--one-file-system}'[Don'\''t cross filesystem boundaries (unix/macOS only).]' \
    '--onedrive-chunk-size[Chunk size to upload files with - must be multiple of 320k (327,680 bytes).]:' \
    '--onedrive-client-id[Microsoft App Client Id]:' \
    '--onedrive-client-secret[Microsoft App Client Secret]:' \
    '--onedrive-drive-id[The ID of the drive to use]:' \
    '--onedrive-drive-type[The type of the drive ( personal | business | documentLibrary )]:' \
    '--onedrive-expose-onenote-files[Set to make OneNote files show up in directory listings.]' \
    '--opendrive-password[Password.]:' \
    '--opendrive-username[Username]:' \
    '--pcloud-client-id[Pcloud App Client Id]:' \
    '--pcloud-client-secret[Pcloud App Client Secret]:' \
    '(-P --progress)'{-P,--progress}'[Show progress during transfer.]' \
    '--qingstor-access-key-id[QingStor Access Key ID]:' \
    '--qingstor-chunk-size[Chunk size to use for uploading.]:' \
    '--qingstor-connection-retries[Number of connection retries.]:' \
    '--qingstor-endpoint[Enter a endpoint URL to connection QingStor API.]:' \
    '--qingstor-env-auth[Get QingStor credentials from runtime. Only applies if access_key_id and secret_access_key is blank.]' \
    '--qingstor-secret-access-key[QingStor Secret Access Key (password)]:' \
    '--qingstor-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--qingstor-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--qingstor-zone[Zone to connect to.]:' \
    '(-q --quiet)'{-q,--quiet}'[Print as little stuff as possible]' \
    '--rc[Enable the remote control server.]' \
    '--rc-addr[IPaddress:Port or :Port to bind server to.]:' \
    '--rc-allow-origin[Set the allowed origin for CORS.]:' \
    '--rc-baseurl[Prefix for URLs - leave blank for root.]:' \
    '--rc-cert[SSL PEM key (concatenation of certificate and CA certificate)]:' \
    '--rc-client-ca[Client certificate authority to verify clients with]:' \
    '--rc-files[Path to local files to serve on the HTTP server.]:' \
    '--rc-htpasswd[htpasswd file - if not provided no authentication is done]:' \
    '--rc-job-expire-duration[expire finished async jobs older than this value]:' \
    '--rc-job-expire-interval[interval to check for expired async jobs]:' \
    '--rc-key[SSL PEM Private key]:' \
    '--rc-max-header-bytes[Maximum size of request header]:' \
    '--rc-no-auth[Don'\''t require auth for certain methods.]' \
    '--rc-pass[Password for authentication.]:' \
    '--rc-realm[realm for authentication]:' \
    '--rc-serve[Enable the serving of remote objects.]' \
    '--rc-server-read-timeout[Timeout for server reading data]:' \
    '--rc-server-write-timeout[Timeout for server writing data]:' \
    '--rc-user[User name for authentication.]:' \
    '--rc-web-fetch-url[URL to fetch the releases for webgui.]:' \
    '--rc-web-gui[Launch WebGUI on localhost]' \
    '--rc-web-gui-update[Update / Force update to latest version of web gui]' \
    '--retries[Retry operations this many times if they fail]:' \
    '--retries-sleep[Interval between retrying operations if they fail, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--s3-access-key-id[AWS Access Key ID.]:' \
    '--s3-acl[Canned ACL used when creating buckets and storing or copying objects.]:' \
    '--s3-bucket-acl[Canned ACL used when creating buckets.]:' \
    '--s3-chunk-size[Chunk size to use for uploading.]:' \
    '--s3-disable-checksum[Don'\''t store MD5 checksum with object metadata]' \
    '--s3-endpoint[Endpoint for S3 API.]:' \
    '--s3-env-auth[Get AWS credentials from runtime (environment variables or EC2/ECS meta data if no env vars).]' \
    '--s3-force-path-style[If true use path style access if false use virtual hosted style.]' \
    '--s3-leave-parts-on-error[If true avoid calling abort upload on a failure, leaving all successfully uploaded parts on S3 for manual recovery.]' \
    '--s3-location-constraint[Location constraint - must be set to match the Region.]:' \
    '--s3-provider[Choose your S3 provider.]:' \
    '--s3-region[Region to connect to.]:' \
    '--s3-secret-access-key[AWS Secret Access Key (password)]:' \
    '--s3-server-side-encryption[The server-side encryption algorithm used when storing this object in S3.]:' \
    '--s3-session-token[An AWS session token]:' \
    '--s3-sse-kms-key-id[If using KMS ID you must provide the ARN of Key.]:' \
    '--s3-storage-class[The storage class to use when storing new objects in S3.]:' \
    '--s3-upload-concurrency[Concurrency for multipart uploads.]:' \
    '--s3-upload-cutoff[Cutoff for switching to chunked upload]:' \
    '--s3-use-accelerate-endpoint[If true use the AWS S3 accelerated endpoint.]' \
    '--s3-v2-auth[If true use v2 authentication.]' \
    '--sftp-ask-password[Allow asking for SFTP password when needed.]' \
    '--sftp-disable-hashcheck[Disable the execution of SSH commands to determine if remote file hashing is available.]' \
    '--sftp-host[SSH host to connect to]:' \
    '--sftp-key-file[Path to PEM-encoded private key file, leave blank or set key-use-agent to use ssh-agent.]:' \
    '--sftp-key-file-pass[The passphrase to decrypt the PEM-encoded private key file.]:' \
    '--sftp-key-use-agent[When set forces the usage of the ssh-agent.]' \
    '--sftp-md5sum-command[The command used to read md5 hashes. Leave blank for autodetect.]:' \
    '--sftp-pass[SSH password, leave blank to use ssh-agent.]:' \
    '--sftp-path-override[Override path used by SSH connection.]:' \
    '--sftp-port[SSH port, leave blank to use default (22)]:' \
    '--sftp-set-modtime[Set the modified time on the remote if set.]' \
    '--sftp-sha1sum-command[The command used to read sha1 hashes. Leave blank for autodetect.]:' \
    '--sftp-use-insecure-cipher[Enable the use of insecure ciphers and key exchange methods.]' \
    '--sftp-user[SSH username, leave blank for current username, brightone]:' \
    '--sharefile-chunk-size[Upload chunk size. Must a power of 2 >= 256k.]:' \
    '--sharefile-endpoint[Endpoint for API calls.]:' \
    '--sharefile-root-folder-id[ID of the root folder]:' \
    '--sharefile-upload-cutoff[Cutoff for switching to multipart upload.]:' \
    '--size-only[Skip based on size only, not mod-time or checksum]' \
    '--skip-links[Don'\''t warn about skipped symlinks.]' \
    '--stats[Interval between printing stats, e.g 500ms, 60s, 5m. (0 to disable)]:' \
    '--stats-file-name-length[Max file name length in stats. 0 for no limit]:' \
    '--stats-log-level[Log level to show --stats output DEBUG|INFO|NOTICE|ERROR]:' \
    '--stats-one-line[Make the stats fit on one line.]' \
    '--stats-one-line-date[Enables --stats-one-line and add current date/time prefix.]' \
    '--stats-one-line-date-format[Enables --stats-one-line-date and uses custom formatted date. Enclose date string in double quotes ("). See https://golang.org/pkg/time/#Time.Format]:' \
    '--stats-unit[Show data rate in stats as either '\''bits'\'' or '\''bytes'\''/s]:' \
    '--streaming-upload-cutoff[Cutoff for switching to chunked upload if file size is unknown. Upload starts after reaching cutoff or when file ends.]:' \
    '--suffix[Suffix to add to changed files.]:' \
    '--suffix-keep-extension[Preserve the extension when using --suffix.]' \
    '--swift-application-credential-id[Application Credential ID (OS_APPLICATION_CREDENTIAL_ID)]:' \
    '--swift-application-credential-name[Application Credential Name (OS_APPLICATION_CREDENTIAL_NAME)]:' \
    '--swift-application-credential-secret[Application Credential Secret (OS_APPLICATION_CREDENTIAL_SECRET)]:' \
    '--swift-auth[Authentication URL for server (OS_AUTH_URL).]:' \
    '--swift-auth-token[Auth Token from alternate authentication - optional (OS_AUTH_TOKEN)]:' \
    '--swift-auth-version[AuthVersion - optional - set to (1,2,3) if your auth URL has no version (ST_AUTH_VERSION)]:' \
    '--swift-chunk-size[Above this size files will be chunked into a _segments container.]:' \
    '--swift-domain[User domain - optional (v3 auth) (OS_USER_DOMAIN_NAME)]:' \
    '--swift-endpoint-type[Endpoint type to choose from the service catalogue (OS_ENDPOINT_TYPE)]:' \
    '--swift-env-auth[Get swift credentials from environment variables in standard OpenStack form.]' \
    '--swift-key[API key or password (OS_PASSWORD).]:' \
    '--swift-no-chunk[Don'\''t chunk files during streaming upload.]' \
    '--swift-region[Region name - optional (OS_REGION_NAME)]:' \
    '--swift-storage-policy[The storage policy to use when creating a new container]:' \
    '--swift-storage-url[Storage URL - optional (OS_STORAGE_URL)]:' \
    '--swift-tenant[Tenant name - optional for v1 auth, this or tenant_id required otherwise (OS_TENANT_NAME or OS_PROJECT_NAME)]:' \
    '--swift-tenant-domain[Tenant domain - optional (v3 auth) (OS_PROJECT_DOMAIN_NAME)]:' \
    '--swift-tenant-id[Tenant ID - optional for v1 auth, this or tenant required otherwise (OS_TENANT_ID)]:' \
    '--swift-user[User name to log in (OS_USERNAME).]:' \
    '--swift-user-id[User ID to log in - optional - most swift systems use user and leave this blank (v3 auth) (OS_USER_ID).]:' \
    '--syslog[Use Syslog for logging]' \
    '--syslog-facility[Facility for syslog, eg KERN,USER,...]:' \
    '--timeout[IO idle timeout]:' \
    '--tpslimit[Limit HTTP transactions per second to this.]:' \
    '--tpslimit-burst[Max burst of transactions for --tpslimit.]:' \
    '--track-renames[When synchronizing, track file renames and do a server side move if possible]' \
    '--transfers[Number of file transfers to run in parallel.]:' \
    '--union-remotes[List of space separated remotes.]:' \
    '(-u --update)'{-u,--update}'[Skip files that are newer on the destination.]' \
    '--use-cookies[Enable session cookiejar.]' \
    '--use-json-log[Use json log format.]' \
    '--use-mmap[Use mmap allocator (see docs).]' \
    '--use-server-modtime[Use server modified time instead of object metadata]' \
    '--user-agent[Set the user-agent to a specified string. The default is rclone/ version]:' \
    '(-v --verbose)'{-v,--verbose}'[Print lots more stuff (repeat for more)]' \
    '--webdav-bearer-token[Bearer token instead of user/pass (eg a Macaroon)]:' \
    '--webdav-bearer-token-command[Command to run to get a bearer token]:' \
    '--webdav-pass[Password.]:' \
    '--webdav-url[URL of http host to connect to]:' \
    '--webdav-user[User name]:' \
    '--webdav-vendor[Name of the Webdav site/service/software you are using]:' \
    '--yandex-client-id[Yandex Client Id]:' \
    '--yandex-client-secret[Yandex Client Secret]:' \
    '--yandex-unlink[Remove existing public link to file/folder with link command rather than creating.]'
}

